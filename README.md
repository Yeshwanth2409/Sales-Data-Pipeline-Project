# ğŸš€ Sales Data Upload & BigQuery Pipeline

This project consists of:
1. **Flask Web App**: Uploads CSV files to Google Cloud Storage (GCS).
2. **Cloud Function**: Automatically loads CSV files from GCS into BigQuery.

---

## ğŸ“ **Project Structure**

ğŸ“‚ sales-data-pipeline/ â”‚â”€â”€ ğŸ“„ functions.py # Cloud Function: Loads CSV files from GCS to BigQuery â”‚â”€â”€ ğŸ“„ main.py # Flask app: Uploads files to GCS â”‚â”€â”€ ğŸ“„ index.html # UI for file uploads â”‚â”€â”€ ğŸ“„ requirements.txt # Dependencies â”‚â”€â”€ ğŸ“„ README.md # Project documentation


---

## ğŸš€ **Setup & Deployment**

### **1ï¸âƒ£ Prerequisites**
Before deploying, ensure you have:
- **Google Cloud SDK** installed â†’ [Install Guide](https://cloud.google.com/sdk/docs/install)
- **BigQuery Dataset** (`sales`) and Table (`orders`)
- **Google Cloud Storage Bucket** (`bkt-sales-data-bucket`)
- **Service Account with necessary permissions** (`roles/storage.admin`, `roles/bigquery.dataEditor`)

---

### **2ï¸âƒ£ Authentication & GCP Setup**
Login and authenticate with GCP:
```sh
gcloud auth login
gcloud config set project [YOUR_PROJECT_ID]
gcloud auth application-default login

Ensure the service account has access to GCS and BigQuery:
gcloud projects add-iam-policy-binding [YOUR_PROJECT_ID] \
    --member="serviceAccount:[YOUR_SA_EMAIL]" \
    --role="roles/storage.admin"

gcloud projects add-iam-policy-binding [YOUR_PROJECT_ID] \
    --member="serviceAccount:[YOUR_SA_EMAIL]" \
    --role="roles/bigquery.dataEditor"

```

### **3ï¸âƒ£ Deploy the Flask Web App (File Uploader)**
This Flask app provides a UI to upload CSV files to GCS.
Install dependencies
```sh
pip install -r requirements.txt
```
Run Flask Locally
```sh
python main.py
```
Open http://127.0.0.1:5000/
Upload a CSV file â†’ It gets stored in GCS Bucket

Deploy Flask App to Cloud Run:
```sh
gcloud builds submit --tag gcr.io/[YOUR_PROJECT_ID]/sales-upload
gcloud run deploy sales-upload \
    --image gcr.io/[YOUR_PROJECT_ID]/sales-upload \
    --platform managed \
    --region [YOUR_REGION] \
    --allow-unauthenticated
```
---
### **4ï¸âƒ£ Deploy Cloud Function (GCS â†’ BigQuery)**
Deploy the function
```sh
gcloud functions deploy hello_gcs \
    --runtime python311 \
    --trigger-resource bkt-sales-data-bucket \
    --trigger-event google.storage.object.finalize \
    --entry-point hello_gcs \
    --region [YOUR_REGION] \
    --allow-unauthenticated

```
---

### **5ï¸âƒ£ Test End-to-End Flow**
Upload a CSV file via Flask UI.
Verify the file appears in GCS Bucket.
Cloud Function triggers automatically and loads data into BigQuery.
Check BigQuery Table:
```sh
SELECT * FROM `your_project.sales.orders`

```

### **âš ï¸ Troubleshooting**
Cloud Function not triggering?
â†’ Ensure GCS events are set up properly.
BigQuery Table missing?
â†’ Run:
```sh
bq mk --table sales.orders column1:STRING,column2:INTEGER,column3:FLOAT
```
Authentication Issues?
â†’ Run gcloud auth application-default login

---
### **ğŸ¯ Next Steps**
Improve UI: Add progress bars for file upload.
Add Logging: Log all processed files in Firestore.
Enhance Security: Restrict public access.

---
### **Author: [Yeshwanth Yedla]**
### **ğŸ“Œ GitHub Repo: [https://github.com/Yeshwanth2409/Sales-Data-Pipeline-Project]**

